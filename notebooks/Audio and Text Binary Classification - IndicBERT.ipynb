{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio and Text Binary Classification - IndicBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxesfL1skK4h"
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install -q -U tensorflow-text\n",
        "!pip install -q tf-models-official"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VlZ0-OZZkPU"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer \n",
        "from transformers import AdamW\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "device = torch.device(\"cuda\")\n",
        "cpu = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpB-LjIdvoFg"
      },
      "source": [
        "data = pd.read_json('/content/drive/MyDrive/mp3_data_w_vectors.json')\n",
        "#data = pd.read_json('https://raw.githubusercontent.com/pradeep-miriyala/multi-modal-bert-models/main/data/song_lyric_map.json?token=ADXRNFRS46PTRG46WUZLXHDBKH7HY')\n",
        "data['iGenre'] = data.apply(lambda x:int(x.Genre=='Devotional'),axis=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQB4oKkaaqWt",
        "outputId": "db0cd53e-bbc3-4a2e-b1b7-74460fa1233a"
      },
      "source": [
        "indic_model = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['predictions.decoder.bias', 'sop_classifier.classifier.weight', 'predictions.decoder.weight', 'sop_classifier.classifier.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmSGgVXklmJc"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeGapwIU_NVE"
      },
      "source": [
        "txt = list(data.apply(lambda x:x.Lyric,axis=1))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO9_dYO5-9eO"
      },
      "source": [
        "sent_id = tokenizer.batch_encode_plus(txt, padding=True, return_token_type_ids=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKTO2aKd14On"
      },
      "source": [
        "max_seq_len = 25\n",
        "all_tokens = tokenizer.batch_encode_plus(txt, max_length=max_seq_len,padding='longest', truncation=True, return_token_type_ids=False)\n",
        "all_seq = torch.tensor(all_tokens['input_ids'])\n",
        "all_mask = torch.tensor(all_tokens['attention_mask'])\n",
        "all_y = torch.tensor(data['iGenre'].tolist())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa1UGyg53sJ_"
      },
      "source": [
        "def get_data_loader(seq,mask,y,batch_size = 32):\n",
        "  data = TensorDataset(seq, mask, y)\n",
        "  sampler = RandomSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "  return (data,sampler,dataloader)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLbr-4KZBoUI"
      },
      "source": [
        "def get_data_loader(seq,mask,y,batch_size = 16):\n",
        "  data = TensorDataset(seq, mask, y)\n",
        "  sampler = RandomSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "  return (data,sampler,dataloader)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrKqt8ZeDKB2"
      },
      "source": [
        "for param in indic_model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN7zC1dHDQ6k"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fc1 = nn.Linear(768,512)      \n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwSiH86W5Epm"
      },
      "source": [
        "def train(model,train_dataloader,loss_fcn):\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):    \n",
        "    if step % 20 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = loss_fcn(preds, labels)\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "    del batch\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgWv4nE75G8O"
      },
      "source": [
        "def evaluate(model,val_dataloader,loss_fcn):\n",
        "  print(\"\\nEvaluating...\")\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):    \n",
        "    if step % 20 == 0 and not step == 0:      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    preds = model(sent_id, mask)\n",
        "    # compute the validation loss between actual and predicted values\n",
        "    loss = loss_fcn(preds,labels)\n",
        "    total_loss = total_loss + loss.item()\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    total_preds.append(preds)\n",
        "    del batch\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtOqcNX64PQO",
        "outputId": "de958b22-0847-4c8d-97b3-bc1aded08bc4"
      },
      "source": [
        "k_folds = 5\n",
        "# number of training epochs\n",
        "epochs = 5\n",
        "torch.manual_seed(42)\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "models = [BERT_Arch(indic_model) for x in range(k_folds)]\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(data)):\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "  train_data, train_sampler, train_dataloader = get_data_loader(all_seq[train_ids],all_mask[train_ids],all_y[train_ids])\n",
        "  test_data, test_sampler, test_dataloader = get_data_loader(all_seq[test_ids],all_mask[test_ids],all_y[test_ids])\n",
        "  best_valid_loss = float('inf')\n",
        "  models[fold].to(device)\n",
        "  class_wts = compute_class_weight('balanced', np.unique(all_y[train_ids].tolist()), all_y[train_ids].tolist())\n",
        "  print(class_wts)\n",
        "  # convert class weights to tensor\n",
        "  weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "  weights = weights.to(device)\n",
        "  # loss function\n",
        "  loss_fcn  = nn.NLLLoss(weight=weights)\n",
        "  # empty lists to store training and validation loss of each epoch\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "  # define the optimizer\n",
        "  optimizer = AdamW(models[fold].parameters(), lr = 1e-5)\n",
        "  #for each epoch\n",
        "  for epoch in range(epochs):     \n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      #train model\n",
        "      train_loss, _ = train(models[fold],train_dataloader,loss_fcn)    \n",
        "      #evaluate model\n",
        "      valid_loss, _ = evaluate(models[fold],test_dataloader,loss_fcn)    \n",
        "      #save the best model\n",
        "      if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(models[fold].state_dict(), 'saved_weights.pt')      \n",
        "      # append training and validation loss\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)    \n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "      torch.cuda.empty_cache()\n",
        "  models[fold].load_state_dict(torch.load('saved_weights.pt'))\n",
        "  preds = models[fold](all_seq[test_ids].to(device), all_mask[test_ids].to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  print('Test')\n",
        "  print(classification_report(all_y[test_ids], preds))\n",
        "  print(pd.crosstab(all_y[test_ids], preds))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "[1.51730104 0.7457483 ]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.695\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.691\n",
            "Validation Loss: 0.689\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.683\n",
            "Validation Loss: 0.681\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.640\n",
            "Validation Loss: 0.633\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.593\n",
            "Validation Loss: 0.626\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.79      0.62        81\n",
            "           1       0.82      0.57      0.67       139\n",
            "\n",
            "    accuracy                           0.65       220\n",
            "   macro avg       0.67      0.68      0.65       220\n",
            "weighted avg       0.71      0.65      0.65       220\n",
            "\n",
            "col_0   0   1\n",
            "row_0        \n",
            "0      64  17\n",
            "1      60  79\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "[1.44719472 0.76393728]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.619\n",
            "Validation Loss: 0.566\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.537\n",
            "Validation Loss: 0.533\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.465\n",
            "Validation Loss: 0.529\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.416\n",
            "Validation Loss: 0.483\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.336\n",
            "Validation Loss: 0.468\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.79      0.75        67\n",
            "           1       0.90      0.86      0.88       153\n",
            "\n",
            "    accuracy                           0.84       220\n",
            "   macro avg       0.81      0.83      0.82       220\n",
            "weighted avg       0.85      0.84      0.84       220\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      53   14\n",
            "1      21  132\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "[1.45364238 0.76215278]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.525\n",
            "Validation Loss: 0.403\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.396\n",
            "Validation Loss: 0.330\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.314\n",
            "Validation Loss: 0.314\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.203\n",
            "Validation Loss: 0.323\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.139\n",
            "Validation Loss: 0.463\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83        68\n",
            "           1       0.94      0.89      0.92       151\n",
            "\n",
            "    accuracy                           0.89       219\n",
            "   macro avg       0.87      0.89      0.88       219\n",
            "weighted avg       0.90      0.89      0.89       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      60    8\n",
            "1      16  135\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "[1.52961672 0.7428088 ]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.440\n",
            "Validation Loss: 0.315\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.250\n",
            "Validation Loss: 0.180\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.159\n",
            "Validation Loss: 0.150\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.120\n",
            "Validation Loss: 0.171\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.108\n",
            "Validation Loss: 0.216\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95        83\n",
            "           1       0.97      0.97      0.97       136\n",
            "\n",
            "    accuracy                           0.96       219\n",
            "   macro avg       0.96      0.96      0.96       219\n",
            "weighted avg       0.96      0.96      0.96       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      79    4\n",
            "1       4  132\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "[1.46822742 0.7582038 ]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.417\n",
            "Validation Loss: 0.240\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.183\n",
            "Validation Loss: 0.137\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.126\n",
            "Validation Loss: 0.122\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.090\n",
            "Validation Loss: 0.120\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.071\n",
            "Validation Loss: 0.137\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97        71\n",
            "           1       0.99      0.98      0.98       148\n",
            "\n",
            "    accuracy                           0.98       219\n",
            "   macro avg       0.97      0.98      0.97       219\n",
            "weighted avg       0.98      0.98      0.98       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      69    2\n",
            "1       3  145\n"
          ]
        }
      ]
    }
  ]
}