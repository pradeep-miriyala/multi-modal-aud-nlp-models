{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio and Text Binary Classification - IndicBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxesfL1skK4h"
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip install sentencepiece -q"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VlZ0-OZZkPU"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from official.nlp import optimization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, AdamW\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "cpu = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpB-LjIdvoFg"
      },
      "source": [
        "data = pd.read_json('/content/drive/MyDrive/mp3_data_w_vectors.json')\n",
        "#data = pd.read_json('https://raw.githubusercontent.com/pradeep-miriyala/multi-modal-bert-models/main/data/song_lyric_map.json?token=ADXRNFRS46PTRG46WUZLXHDBKH7HY')\n",
        "data['iGenre'] = data.apply(lambda x:int(x.Genre=='Devotional'),axis=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQB4oKkaaqWt",
        "outputId": "6f2d9bde-c4b8-4dba-8247-06885f60eac5"
      },
      "source": [
        "indic_model = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
            "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'sop_classifier.classifier.weight', 'sop_classifier.classifier.bias', 'predictions.dense.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmSGgVXklmJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60f8748-7455-499c-9615-b3d2914458e8"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeGapwIU_NVE"
      },
      "source": [
        "txt = list(data.apply(lambda x:x.Lyric,axis=1))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO9_dYO5-9eO"
      },
      "source": [
        "sent_id = tokenizer.batch_encode_plus(txt, padding=True, return_token_type_ids=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKTO2aKd14On"
      },
      "source": [
        "max_seq_len = 25\n",
        "all_tokens = tokenizer.batch_encode_plus(txt, max_length=max_seq_len,padding='longest', truncation=True, return_token_type_ids=False)\n",
        "all_seq = torch.tensor(all_tokens['input_ids'])\n",
        "all_mask = torch.tensor(all_tokens['attention_mask'])\n",
        "all_y = torch.tensor(data['iGenre'].tolist())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLbr-4KZBoUI"
      },
      "source": [
        "def get_data_loader(seq, mask, y, mfcc_data=None,batch_size = 16):\n",
        "  data = TensorDataset(seq, mask, mfcc_data, y)\n",
        "  sampler = RandomSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "  return (data,sampler,dataloader)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrKqt8ZeDKB2"
      },
      "source": [
        "for param in indic_model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN7zC1dHDQ6k"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert, fusion=False):      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fusion = fusion\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      if self.fusion:\n",
        "        self.fc2 = nn.Linear(512,512)\n",
        "        self.fca1 = nn.Linear(14,64)\n",
        "        self.fca2 = nn.Linear(64,128)\n",
        "        self.fusion1 = nn.Linear(640,512) # 512 + 128\n",
        "        self.fusion2 = nn.Linear(512,2)\n",
        "      else:\n",
        "        self.fc2 = nn.Linear(512,2)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask, mfcc_data):\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      if self.fusion:\n",
        "        a1 = self.fca1(mfcc_data)\n",
        "        a1 = self.relu(a1)\n",
        "        a1 = self.dropout(a1)\n",
        "        a1 = self.fca2(a1)\n",
        "        x = self.relu(x) # Activation for output from text features\n",
        "        x = torch.cat((x,a1),dim=1) # Fusion Layer\n",
        "        x = self.fusion1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fusion2(x)\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwSiH86W5Epm"
      },
      "source": [
        "def train(model,train_dataloader,loss_fcn,optimizer):\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):    \n",
        "    if step % 20 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, mfcc_means, labels = batch\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask, mfcc_means)\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = loss_fcn(preds, labels)\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "    del batch\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgWv4nE75G8O"
      },
      "source": [
        "def evaluate(model,val_dataloader,loss_fcn):\n",
        "  print(\"\\nEvaluating...\")\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):    \n",
        "    if step % 20 == 0 and not step == 0:      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    sent_id, mask, mfcc_means, labels = batch\n",
        "    preds = model(sent_id, mask, mfcc_means)\n",
        "    # compute the validation loss between actual and predicted values\n",
        "    loss = loss_fcn(preds,labels)\n",
        "    total_loss = total_loss + loss.item()\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    total_preds.append(preds)\n",
        "    del batch\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtOqcNX64PQO"
      },
      "source": [
        "fusion = True\n",
        "k_folds = 5\n",
        "# number of training epochs\n",
        "epochs = 5\n",
        "torch.manual_seed(42)\n",
        "def run_models(fusion):\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=True,random_state=42)\n",
        "  models = [BERT_Arch(indic_model,fusion) for x in range(k_folds)]\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(data)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "    train_mfcc = torch.tensor([[_ for _ in data['mfcc_mean'].iloc[x]] for x in train_ids])\n",
        "    test_mfcc = torch.tensor([[_ for _ in data['mfcc_mean'].iloc[x]] for x in test_ids])\n",
        "    train_data, train_sampler, train_dataloader = get_data_loader(all_seq[train_ids],all_mask[train_ids],all_y[train_ids],train_mfcc)\n",
        "    test_data, test_sampler, test_dataloader = get_data_loader(all_seq[test_ids],all_mask[test_ids],all_y[test_ids],test_mfcc)\n",
        "    best_valid_loss = float('inf')\n",
        "    models[fold].to(device)\n",
        "    class_wts = compute_class_weight('balanced', np.unique(all_y[train_ids].tolist()), all_y[train_ids].tolist())\n",
        "    print(class_wts)\n",
        "    # convert class weights to tensor\n",
        "    weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "    weights = weights.to(device)\n",
        "    # loss function\n",
        "    loss_fcn  = nn.NLLLoss(weight=weights)\n",
        "    # empty lists to store training and validation loss of each epoch\n",
        "    train_losses=[]\n",
        "    valid_losses=[]\n",
        "    # define the optimizer\n",
        "    optimizer = AdamW(models[fold].parameters(), lr = 1e-5)\n",
        "    #for each epoch\n",
        "    for epoch in range(epochs):     \n",
        "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "        #train model\n",
        "        train_loss, _ = train(models[fold],train_dataloader,loss_fcn,optimizer)    \n",
        "        #evaluate model\n",
        "        valid_loss, _ = evaluate(models[fold],test_dataloader,loss_fcn)    \n",
        "        #save the best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(models[fold].state_dict(), 'saved_weights.pt')      \n",
        "        # append training and validation loss\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)    \n",
        "        print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "        print(f'Validation Loss: {valid_loss:.3f}')\n",
        "        torch.cuda.empty_cache()\n",
        "    models[fold].load_state_dict(torch.load('saved_weights.pt'))    \n",
        "    preds = models[fold](all_seq[test_ids].to(device), all_mask[test_ids].to(device), test_mfcc.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    preds = np.argmax(preds, axis = 1)\n",
        "    print('Test')\n",
        "    print(classification_report(all_y[test_ids], preds))\n",
        "    print(pd.crosstab(all_y[test_ids], preds))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Oy7WGRbP7mL",
        "outputId": "471e69af-4a73-4baa-b718-16e0aaaf1359"
      },
      "source": [
        "# Fusion Model\n",
        "run_models(True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "[1.47147651 0.75734024]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.695\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.692\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.691\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.676\n",
            "Validation Loss: 0.657\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.51      0.54        72\n",
            "           1       0.78      0.82      0.80       148\n",
            "\n",
            "    accuracy                           0.72       220\n",
            "   macro avg       0.68      0.67      0.67       220\n",
            "weighted avg       0.71      0.72      0.71       220\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      37   35\n",
            "1      27  121\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "[1.48141892 0.75473322]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.686\n",
            "Validation Loss: 0.672\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.670\n",
            "Validation Loss: 0.659\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.637\n",
            "Validation Loss: 0.611\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.604\n",
            "Validation Loss: 0.614\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.563\n",
            "Validation Loss: 0.582\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.73      0.68        74\n",
            "           1       0.85      0.78      0.81       146\n",
            "\n",
            "    accuracy                           0.76       220\n",
            "   macro avg       0.74      0.76      0.74       220\n",
            "weighted avg       0.78      0.76      0.77       220\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      54   20\n",
            "1      32  114\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "[1.46822742 0.7582038 ]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.675\n",
            "Validation Loss: 0.657\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.624\n",
            "Validation Loss: 0.584\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.577\n",
            "Validation Loss: 0.546\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.515\n",
            "Validation Loss: 0.542\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.459\n",
            "Validation Loss: 0.656\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.69      0.70        71\n",
            "           1       0.85      0.86      0.86       148\n",
            "\n",
            "    accuracy                           0.80       219\n",
            "   macro avg       0.78      0.77      0.78       219\n",
            "weighted avg       0.80      0.80      0.80       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      49   22\n",
            "1      21  127\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "[1.48310811 0.75429553]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.677\n",
            "Validation Loss: 0.651\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.612\n",
            "Validation Loss: 0.573\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.546\n",
            "Validation Loss: 0.562\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.458\n",
            "Validation Loss: 0.487\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.414\n",
            "Validation Loss: 0.523\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.72      0.80        74\n",
            "           1       0.87      0.96      0.91       145\n",
            "\n",
            "    accuracy                           0.88       219\n",
            "   macro avg       0.88      0.84      0.85       219\n",
            "weighted avg       0.88      0.88      0.87       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      53   21\n",
            "1       6  139\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "[1.50859107 0.74787053]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.666\n",
            "Validation Loss: 0.618\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.589\n",
            "Validation Loss: 0.520\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.505\n",
            "Validation Loss: 0.414\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.415\n",
            "Validation Loss: 0.323\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.349\n",
            "Validation Loss: 0.282\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.95      0.87        79\n",
            "           1       0.97      0.87      0.92       140\n",
            "\n",
            "    accuracy                           0.90       219\n",
            "   macro avg       0.89      0.91      0.89       219\n",
            "weighted avg       0.91      0.90      0.90       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      75    4\n",
            "1      18  122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YYY3JB4P-hY",
        "outputId": "427f9c44-3316-4159-f260-0db1b33190ee"
      },
      "source": [
        "# Text Only model\n",
        "run_models(False)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "[1.47147651 0.75734024]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.482\n",
            "Validation Loss: 0.347\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.320\n",
            "Validation Loss: 0.316\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.260\n",
            "Validation Loss: 0.346\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.235\n",
            "Validation Loss: 0.376\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.197\n",
            "Validation Loss: 0.374\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86        72\n",
            "           1       0.94      0.92      0.93       148\n",
            "\n",
            "    accuracy                           0.91       220\n",
            "   macro avg       0.89      0.90      0.90       220\n",
            "weighted avg       0.91      0.91      0.91       220\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      64    8\n",
            "1      12  136\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "[1.48141892 0.75473322]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.416\n",
            "Validation Loss: 0.317\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.236\n",
            "Validation Loss: 0.288\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.208\n",
            "Validation Loss: 0.275\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.174\n",
            "Validation Loss: 0.322\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.138\n",
            "Validation Loss: 0.318\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88        74\n",
            "           1       0.96      0.92      0.94       146\n",
            "\n",
            "    accuracy                           0.92       220\n",
            "   macro avg       0.90      0.92      0.91       220\n",
            "weighted avg       0.92      0.92      0.92       220\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      68    6\n",
            "1      12  134\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "[1.46822742 0.7582038 ]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.378\n",
            "Validation Loss: 0.211\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.199\n",
            "Validation Loss: 0.145\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.153\n",
            "Validation Loss: 0.154\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.140\n",
            "Validation Loss: 0.151\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.107\n",
            "Validation Loss: 0.173\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.95        71\n",
            "           1       0.99      0.96      0.97       148\n",
            "\n",
            "    accuracy                           0.96       219\n",
            "   macro avg       0.95      0.97      0.96       219\n",
            "weighted avg       0.96      0.96      0.96       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      69    2\n",
            "1       6  142\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "[1.48310811 0.75429553]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.347\n",
            "Validation Loss: 0.179\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.151\n",
            "Validation Loss: 0.119\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.097\n",
            "Validation Loss: 0.126\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.072\n",
            "Validation Loss: 0.122\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.072\n",
            "Validation Loss: 0.159\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.95        74\n",
            "           1       0.98      0.97      0.98       145\n",
            "\n",
            "    accuracy                           0.97       219\n",
            "   macro avg       0.96      0.97      0.96       219\n",
            "weighted avg       0.97      0.97      0.97       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      71    3\n",
            "1       4  141\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "[1.50859107 0.74787053]\n",
            "\n",
            " Epoch 1 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.384\n",
            "Validation Loss: 0.158\n",
            "\n",
            " Epoch 2 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.146\n",
            "Validation Loss: 0.077\n",
            "\n",
            " Epoch 3 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.090\n",
            "Validation Loss: 0.049\n",
            "\n",
            " Epoch 4 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.063\n",
            "Validation Loss: 0.037\n",
            "\n",
            " Epoch 5 / 5\n",
            "  Batch    20  of     55.\n",
            "  Batch    40  of     55.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.044\n",
            "Validation Loss: 0.069\n",
            "Test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98        79\n",
            "           1       0.99      0.99      0.99       140\n",
            "\n",
            "    accuracy                           0.99       219\n",
            "   macro avg       0.98      0.99      0.99       219\n",
            "weighted avg       0.99      0.99      0.99       219\n",
            "\n",
            "col_0   0    1\n",
            "row_0         \n",
            "0      78    1\n",
            "1       2  138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVIvoFZnT72c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}