{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import Counter, defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\ngpu = torch.device('cuda')\ncpu = torch.device('cpu')","metadata":{"id":"4CnfqS_MEaQ2","execution":{"iopub.status.busy":"2021-10-03T14:26:35.084114Z","iopub.execute_input":"2021-10-03T14:26:35.085088Z","iopub.status.idle":"2021-10-03T14:26:36.242861Z","shell.execute_reply.started":"2021-10-03T14:26:35.084956Z","shell.execute_reply":"2021-10-03T14:26:36.242142Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#data = pd.read_json('/content/drive/MyDrive/song_lyric_map.json')\ndata = pd.read_json('../input/annamayya-song-lyrical-map/song_lyric_map.json')\ndata['iGenre'] = data.apply(lambda x:int(x.Genre=='Devotional'),axis=1)","metadata":{"id":"94ImUWYCEnYK","execution":{"iopub.status.busy":"2021-10-03T14:26:36.244393Z","iopub.execute_input":"2021-10-03T14:26:36.244638Z","iopub.status.idle":"2021-10-03T14:26:36.418243Z","shell.execute_reply.started":"2021-10-03T14:26:36.244607Z","shell.execute_reply":"2021-10-03T14:26:36.417530Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def build_word_tokenizer(sentences, word_threshold=1):\n    word_sequences = [[] for _ in sentences]\n    # Dictionary to create word to frequency\n    word_counter = Counter()\n    for i, sentence in enumerate(sentences):\n        for word in sentence.split():\n            word_counter.update([word])\n            word_sequences[i].append(word)\n    word_counter = {k: v for k, v in word_counter.items() if v > word_threshold}\n    word_counter = sorted(word_counter, key=word_counter.get, reverse=True)\n    word2idx = defaultdict(int)\n    idx2word = defaultdict(str)\n    for i, word in enumerate(word_counter):\n        word2idx[word] = i\n        idx2word[i] = word\n    return word_counter, word_sequences, word2idx, idx2word\n\n\ndef pad_input(sent_sequence, seq_len):\n    features = np.zeros((len(sent_sequence), seq_len), dtype=int)\n    for i, sentence in enumerate(sent_sequence):\n        if len(sentence) != 0:\n            features[i, -len(sentence):] = np.array(sentence)[:seq_len]\n    return features\n\n\ndef tokenize(sentences, word2idx, seq_len=None):\n    token_matrix = [[] for _ in sentences]\n    for i, sentence in enumerate(sentences):\n        token_matrix[i] = [word2idx[word] for word in sentence.split()]\n    if seq_len:\n      token_matrix = pad_input(token_matrix, seq_len)\n    return token_matrix","metadata":{"id":"XuFsfJUEP2hd","execution":{"iopub.status.busy":"2021-10-03T14:26:36.420194Z","iopub.execute_input":"2021-10-03T14:26:36.420602Z","iopub.status.idle":"2021-10-03T14:26:36.432145Z","shell.execute_reply.started":"2021-10-03T14:26:36.420565Z","shell.execute_reply":"2021-10-03T14:26:36.431391Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def plot_results(results, model_name):\n    fig = plt.figure(figsize=[20, 10])\n    epochs = len(results[0]['train_precision'])\n    x_label = f'{len(results)} Fold and {epochs} Epochs'\n    legend_labels = ['Train', 'Validation']\n\n    def subplot_routine(key1, key2, title, loss=False):\n        plt.plot([x for k in results for x in results[k][key1]])\n        plt.plot([x for k in results for x in results[k][key2]])\n        plt.grid()\n        plt.xlabel(x_label)\n        plt.title(title)\n        plt.legend(legend_labels)\n        if not loss: \n            plt.ylim([0, 1.1])\n        else:\n            b, t = plt.ylim()\n            plt.ylim(np.floor(b), np.ceil(t))\n    \n    gs = GridSpec(2, 3, figure=fig)\n    plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n    subplot_routine('train_losses', 'validation_losses', 'Losses', True)\n    plt.subplot(2, 3, 4)\n    subplot_routine('train_precision', 'validation_precision', 'Precision')\n    plt.subplot(2, 3, 5)\n    subplot_routine('train_recall', 'validation_recall', 'Recall')\n    plt.subplot(2, 3, 6)\n    subplot_routine('train_f1', 'validation_f1', 'F1')\n    plt.suptitle(f'Metrics for {model_name}')\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"U4vaAZK_0gZ9","execution":{"iopub.status.busy":"2021-10-03T14:26:36.434443Z","iopub.execute_input":"2021-10-03T14:26:36.434644Z","iopub.status.idle":"2021-10-03T14:26:36.449957Z","shell.execute_reply.started":"2021-10-03T14:26:36.434622Z","shell.execute_reply":"2021-10-03T14:26:36.449201Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Ref : https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/ \nclass LstmModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, seq_len, fusion=False, n_layers=6, drop_prob=0.1):\n        super(LstmModel, self).__init__()\n        self.n_layers = 6\n        self.hidden_dim = hidden_dim\n        self.fusion = fusion\n        self.seq_len = seq_len\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, self.n_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n        self.dropout = nn.Dropout(drop_prob)\n        if self.fusion:\n            self.fc = nn.Linear(2*hidden_dim, 512)\n            self.fca = [nn.Linear(41,64), nn.ReLU(), nn.Linear(64,128), nn.ReLU()]\n            self.fusion = [nn.Linear(128+512,512), nn.ReLU(), nn.Linear(512, 2), nn.ReLU()] # 128 + 512\n        else:\n            self.fc = nn.Linear(2*hidden_dim, 2)\n        self.softmax = nn.LogSoftmax(dim=0)\n        \n    def forward(self, x, mfcc, hidden):\n        batch_size = x.size(0)\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        out_forward = lstm_out[range(len(lstm_out)), self.seq_len - 1, :self.hidden_dim]\n        out_reverse = lstm_out[:, 0, self.hidden_dim:]\n        out_reduced = torch.cat((out_forward, out_reverse), 1)\n        out = self.dropout(out_reduced)\n        out = self.fc(out)\n        out = torch.relu_(out)\n        if self.fusion:\n            a = mfcc\n            for layer in self.fca:\n                layer.to(gpu)\n                a = layer(a)\n            \n            out = torch.cat((out,a), dim=1)\n            for layer in self.fusion:\n                layer.to(gpu)\n                out = layer(out)            \n        out = self.softmax(out)\n        out = out.view(batch_size, -1)\n        return out, hidden\n    \n    def init_hidden(self, batch_size, target_device):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_().to(target_device),\n                      weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_().to(target_device))\n        return hidden","metadata":{"id":"SInhoF2jPM4S","execution":{"iopub.status.busy":"2021-10-03T14:26:36.452732Z","iopub.execute_input":"2021-10-03T14:26:36.453244Z","iopub.status.idle":"2021-10-03T14:26:36.477430Z","shell.execute_reply.started":"2021-10-03T14:26:36.453211Z","shell.execute_reply":"2021-10-03T14:26:36.476670Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def run_model(model, data_loader, loss_fcn, optimizer, target_device, is_training, clip_at=10):\n    if is_training:\n        model.train()\n    else:\n        model.eval()\n    total_loss, total_accuracy = 0, 0\n    # empty list to save model predictions\n    model_predictions, model_labels = [], []\n    # iterate over batches\n    for step, batch in enumerate(data_loader):\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(data_loader)))\n        # push the batch to gpu\n        batch = [r.to(target_device) for r in batch]\n        sent_vectors, mfcc_means, labels = batch\n        h = model.init_hidden(len(labels), target_device)\n        if is_training:\n            model.zero_grad()  # clear previously calculated gradients\n            # get model predictions for the current batch\n            predictions, tmp1 = model(sent_vectors, mfcc_means, h)\n            del tmp1\n        else:\n            with torch.no_grad(): \n                predictions, tmp1 = model(sent_vectors, mfcc_means, h)\n                del tmp1        \n        loss = loss_fcn(predictions, labels)\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n        if is_training:\n            loss.backward()  # backward pass to calculate the gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_at)\n            # update parameters\n            optimizer.step()\n        predictions = predictions.detach().cpu().numpy()\n        # append the model predictions\n        model_predictions.append(predictions)\n        model_labels.append(labels.detach().cpu().numpy())\n        del batch\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(data_loader)\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    model_predictions = np.concatenate(model_predictions, axis=0)\n    model_labels = np.concatenate(model_labels, axis=0)\n    model_predictions = np.argmax(model_predictions, axis=1)\n    return avg_loss, model_predictions, model_labels, model, optimizer","metadata":{"id":"bFmMrnNoeEmg","execution":{"iopub.status.busy":"2021-10-03T14:26:36.478834Z","iopub.execute_input":"2021-10-03T14:26:36.479245Z","iopub.status.idle":"2021-10-03T14:26:36.493067Z","shell.execute_reply.started":"2021-10-03T14:26:36.479211Z","shell.execute_reply":"2021-10-03T14:26:36.492409Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def model_execution(fusion):\n    torch.manual_seed(42)\n    \n    X_train, X_test, y_train, y_test = train_test_split(data[['Lyric','mfcc_mean']],data['iGenre'],random_state=42, test_size=0.3)\n    word_counter, word_sequences, word2idx, idx2word = build_word_tokenizer(X_train['Lyric'], 5)\n    \n    vocab_size = len(word2idx) + 1\n    embedding_dim = 256\n    hidden_dim = 128\n    lr = 5e-5\n    results = {x: {} for x in range(1)}\n    fold = 0\n    batch_size = 16\n    epochs = 15\n\n    \n    best_valid_loss = float('inf')    \n    best_train_predictions, best_test_predictions, best_train_labels, best_test_labels = [], [], [], []\n    results[fold]['train_precision'] = []\n    results[fold]['train_recall'] = []\n    results[fold]['train_f1'] = []\n    results[fold]['validation_precision'] = []\n    results[fold]['validation_recall'] = []\n    results[fold]['validation_f1'] = []\n    train_losses, valid_losses = [], []\n\n    seq_len = 300\n    train_tokens = tokenize(X_train['Lyric'], word2idx, seq_len=seq_len)\n    test_tokens = tokenize(X_test['Lyric'], word2idx, seq_len=seq_len)\n    \n    train_mfcc = torch.tensor([[_ for _ in x] for x in X_train['mfcc_mean']])\n    test_mfcc = torch.tensor([[_ for _ in x] for x in X_test['mfcc_mean']])\n\n    train_data = TensorDataset(torch.from_numpy(train_tokens), train_mfcc, torch.from_numpy(y_train.to_numpy()))\n    test_data = TensorDataset(torch.from_numpy(test_tokens), test_mfcc, torch.from_numpy(y_test.to_numpy()))\n\n    train_data_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n    test_data_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n    \n    loss_fcn = nn.NLLLoss()\n    model = LstmModel(vocab_size, embedding_dim, hidden_dim, seq_len, fusion)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    model.to(gpu)\n    \n    for epoch in range(epochs):\n        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n        # train model\n        train_loss, train_predictions, train_labels, model, optimizer = run_model(model, train_data_loader, loss_fcn, optimizer,\n                                                                gpu, True)\n        # evaluate model\n        valid_loss, test_predictions, test_labels, model, optimizer = run_model(model, test_data_loader, loss_fcn, optimizer,\n                                                              gpu, False)\n        # save the best model\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            best_train_predictions = train_predictions\n            best_test_predictions = test_predictions\n            best_train_labels = train_labels\n            best_test_labels = test_labels\n            torch.save(model.state_dict(), f'saved_weights_Fold{fold}.pt')\n            # append training and validation loss\n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        print(f'Losses - Train:{train_loss:.3f} / Validation:{valid_loss:.3f}')\n        results[fold]['train_precision'].append(precision_score(train_labels, train_predictions))\n        results[fold]['train_recall'].append(recall_score(train_labels, train_predictions))\n        results[fold]['train_f1'].append(f1_score(train_labels, train_predictions))\n        results[fold]['validation_precision'].append(precision_score(test_labels, test_predictions))\n        results[fold]['validation_recall'].append(recall_score(test_labels, test_predictions))\n        results[fold]['validation_f1'].append(f1_score(test_labels, test_predictions))\n        torch.cuda.empty_cache()\n    print('On Train Data')\n    print(classification_report(best_train_labels, best_train_predictions))\n    print('On Test Data')\n    print(classification_report(best_test_labels, best_test_predictions))\n    results[fold]['train_losses'] = train_losses\n    results[fold]['validation_losses'] = valid_losses\n    return results","metadata":{"id":"T8e5ZCXjzdjj","outputId":"ed519bc0-475a-4fa9-f3a4-cd356ae93ea2","execution":{"iopub.status.busy":"2021-10-03T14:26:36.494405Z","iopub.execute_input":"2021-10-03T14:26:36.495091Z","iopub.status.idle":"2021-10-03T14:26:36.516744Z","shell.execute_reply.started":"2021-10-03T14:26:36.495057Z","shell.execute_reply":"2021-10-03T14:26:36.516035Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"lstm_text_only = model_execution(False)\nplot_results(lstm_text_only, 'LSTM Text Classification')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T14:26:36.517850Z","iopub.execute_input":"2021-10-03T14:26:36.518182Z","iopub.status.idle":"2021-10-03T14:30:06.559597Z","shell.execute_reply.started":"2021-10-03T14:26:36.518133Z","shell.execute_reply":"2021-10-03T14:30:06.558891Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"lstm_fusion = model_execution(True)\nplot_results(lstm_fusion, 'LSTM Fusion Results')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T14:30:06.560573Z","iopub.execute_input":"2021-10-03T14:30:06.560805Z","iopub.status.idle":"2021-10-03T14:33:37.665022Z","shell.execute_reply.started":"2021-10-03T14:30:06.560774Z","shell.execute_reply":"2021-10-03T14:33:37.664338Z"},"trusted":true},"execution_count":9,"outputs":[]}]}