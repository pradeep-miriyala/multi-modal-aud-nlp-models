{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import Counter, defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport matplotlib.pyplot as plt\n\ngpu = torch.device('cuda')\ncpu = torch.device('cpu')","metadata":{"id":"4CnfqS_MEaQ2","execution":{"iopub.status.busy":"2021-10-03T08:53:29.776409Z","iopub.execute_input":"2021-10-03T08:53:29.776930Z","iopub.status.idle":"2021-10-03T08:53:35.608908Z","shell.execute_reply.started":"2021-10-03T08:53:29.776715Z","shell.execute_reply":"2021-10-03T08:53:35.607851Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#data = pd.read_json('/content/drive/MyDrive/song_lyric_map.json')\ndata = pd.read_json('../input/annamayya-song-lyrical-map/song_lyric_map.json')\ndata['iGenre'] = data.apply(lambda x:int(x.Genre=='Devotional'),axis=1)","metadata":{"id":"94ImUWYCEnYK","execution":{"iopub.status.busy":"2021-10-03T08:53:36.691237Z","iopub.execute_input":"2021-10-03T08:53:36.692086Z","iopub.status.idle":"2021-10-03T08:53:37.172603Z","shell.execute_reply.started":"2021-10-03T08:53:36.692047Z","shell.execute_reply":"2021-10-03T08:53:37.169675Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data['Lyric'],data['iGenre'],random_state=42, test_size=0.3)","metadata":{"id":"POd1ZwaiMhsc","execution":{"iopub.status.busy":"2021-10-03T08:53:39.292581Z","iopub.execute_input":"2021-10-03T08:53:39.293745Z","iopub.status.idle":"2021-10-03T08:53:39.304009Z","shell.execute_reply.started":"2021-10-03T08:53:39.293698Z","shell.execute_reply":"2021-10-03T08:53:39.302952Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def build_word_tokenizer(sentences, word_threshold=1):\n    word_sequences = [[] for _ in sentences]\n    # Dictionary to create word to frequency\n    word_counter = Counter()\n    for i, sentence in enumerate(sentences):\n        for word in sentence.split():\n            word_counter.update([word])\n            word_sequences[i].append(word)\n    word_counter = {k: v for k, v in word_counter.items() if v > word_threshold}\n    word_counter = sorted(word_counter, key=word_counter.get, reverse=True)\n    word2idx = defaultdict(int)\n    idx2word = defaultdict(str)\n    for i, word in enumerate(word_counter):\n        word2idx[word] = i\n        idx2word[i] = word\n    return word_counter, word_sequences, word2idx, idx2word\n\n\ndef pad_input(sent_sequence, seq_len):\n    features = np.zeros((len(sent_sequence), seq_len), dtype=int)\n    for i, sentence in enumerate(sent_sequence):\n        if len(sentence) != 0:\n            features[i, -len(sentence):] = np.array(sentence)[:seq_len]\n    return features\n\n\ndef tokenize(sentences, word2idx, seq_len=None):\n    token_matrix = [[] for _ in sentences]\n    for i, sentence in enumerate(sentences):\n        token_matrix[i] = [word2idx[word] for word in sentence.split()]\n    if seq_len:\n      token_matrix = pad_input(token_matrix, seq_len)\n    return token_matrix","metadata":{"id":"XuFsfJUEP2hd","execution":{"iopub.status.busy":"2021-10-03T08:53:40.270841Z","iopub.execute_input":"2021-10-03T08:53:40.271687Z","iopub.status.idle":"2021-10-03T08:53:40.297505Z","shell.execute_reply.started":"2021-10-03T08:53:40.271650Z","shell.execute_reply":"2021-10-03T08:53:40.296019Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"word_counter, word_sequences, word2idx, idx2word = build_word_tokenizer(X_train, 5)","metadata":{"id":"6P87L2UicDDc","execution":{"iopub.status.busy":"2021-10-03T08:53:41.403194Z","iopub.execute_input":"2021-10-03T08:53:41.403524Z","iopub.status.idle":"2021-10-03T08:53:41.522038Z","shell.execute_reply.started":"2021-10-03T08:53:41.403494Z","shell.execute_reply":"2021-10-03T08:53:41.520924Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"seq_len = 300\ntrain_tokens = tokenize(X_train, word2idx, seq_len=seq_len)\ntest_tokens = tokenize(X_test, word2idx, seq_len=seq_len)","metadata":{"id":"cq-oQocLcQKd","execution":{"iopub.status.busy":"2021-10-03T09:43:07.970096Z","iopub.execute_input":"2021-10-03T09:43:07.970391Z","iopub.status.idle":"2021-10-03T09:43:08.018177Z","shell.execute_reply.started":"2021-10-03T09:43:07.970363Z","shell.execute_reply":"2021-10-03T09:43:08.017088Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"train_data = TensorDataset(torch.from_numpy(train_tokens), torch.from_numpy(y_train.to_numpy()))\ntest_data = TensorDataset(torch.from_numpy(test_tokens), torch.from_numpy(y_test.to_numpy()))","metadata":{"id":"rDglKeiQN2iq","execution":{"iopub.status.busy":"2021-10-03T09:43:09.303161Z","iopub.execute_input":"2021-10-03T09:43:09.303505Z","iopub.status.idle":"2021-10-03T09:43:09.309158Z","shell.execute_reply.started":"2021-10-03T09:43:09.303460Z","shell.execute_reply":"2021-10-03T09:43:09.307999Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"def plot_results(results, model_name):\n    plt.figure(figsize=[20, 5])\n    epochs = len(results[0]['train_precision'])\n    x_label = f'{len(results)} Fold and {epochs} Epochs'\n    legend_labels = ['Train', 'Validation']\n\n    def subplot_routine(key1, key2, title):\n        plt.plot([x for k in results for x in results[k][key1]])\n        plt.plot([x for k in results for x in results[k][key2]])\n        plt.grid()\n        plt.xlabel(x_label)\n        plt.title(title)\n        plt.legend(legend_labels)\n        plt.ylim([0, 1.1])\n\n    plt.subplot(1, 3, 1)\n    subplot_routine('train_precision', 'validation_precision', 'Precision')\n    plt.subplot(1, 3, 2)\n    subplot_routine('train_recall', 'validation_recall', 'Recall')\n    plt.subplot(1, 3, 3)\n    subplot_routine('train_f1', 'validation_f1', 'F1')\n    plt.suptitle(f'Metrics for {model_name}')\n    plt.show()","metadata":{"id":"U4vaAZK_0gZ9","execution":{"iopub.status.busy":"2021-10-03T09:19:02.432289Z","iopub.execute_input":"2021-10-03T09:19:02.433499Z","iopub.status.idle":"2021-10-03T09:19:02.449286Z","shell.execute_reply.started":"2021-10-03T09:19:02.433453Z","shell.execute_reply":"2021-10-03T09:19:02.448269Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# Ref : https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/ \nclass LstmModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, drop_prob=0.1):\n        super(LstmModel, self).__init__()\n        self.n_layers = 6\n        self.hidden_dim = hidden_dim        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, self.n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, 2)\n        self.softmax = nn.LogSoftmax(dim=0)\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:,-1,:])\n        out = torch.relu_(out)\n        out = self.softmax(out)\n        out = out.view(batch_size, -1)\n        return out, hidden\n    \n    def init_hidden(self, batch_size, target_device):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(target_device),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(target_device))\n        return hidden","metadata":{"id":"SInhoF2jPM4S","execution":{"iopub.status.busy":"2021-10-03T09:43:21.561681Z","iopub.execute_input":"2021-10-03T09:43:21.562409Z","iopub.status.idle":"2021-10-03T09:43:21.576879Z","shell.execute_reply.started":"2021-10-03T09:43:21.562378Z","shell.execute_reply":"2021-10-03T09:43:21.574504Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"def run_model(model, data_loader, loss_fcn, optimizer, target_device, is_training, clip_at=10):\n    if is_training:\n        model.train()\n    else:\n        model.eval()\n    model.to(target_device)\n    total_loss, total_accuracy = 0, 0\n    # empty list to save model predictions\n    model_predictions, model_labels = [], []\n    # iterate over batches\n    for step, batch in enumerate(data_loader):\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(data_loader)))\n        # push the batch to gpu\n        batch = [r.to(target_device) for r in batch]\n        sent_vectors, labels = batch\n        h = model.init_hidden(len(labels), target_device)\n        if is_training:\n            model.zero_grad()  # clear previously calculated gradients\n            # get model predictions for the current batch\n            predictions, tmp1 = model(sent_vectors, h)\n            del tmp1\n        else:\n            with torch.no_grad(): \n                predictions, tmp1 = model(sent_vectors, h)\n                del tmp1        \n        loss = loss_fcn(predictions, labels)\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n        if is_training:\n            loss.backward()  # backward pass to calculate the gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_at)\n            # update parameters\n            optimizer.step()\n        predictions = predictions.detach().cpu().numpy()\n        # append the model predictions\n        model_predictions.append(predictions)\n        model_labels.append(labels.detach().cpu().numpy())\n        del batch\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(data_loader)\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    model_predictions = np.concatenate(model_predictions, axis=0)\n    model_labels = np.concatenate(model_labels, axis=0)\n    model_predictions = np.argmax(model_predictions, axis=1)\n    return avg_loss, model_predictions, model_labels, model, optimizer","metadata":{"id":"bFmMrnNoeEmg","execution":{"iopub.status.busy":"2021-10-03T09:43:24.126366Z","iopub.execute_input":"2021-10-03T09:43:24.126713Z","iopub.status.idle":"2021-10-03T09:43:24.140637Z","shell.execute_reply.started":"2021-10-03T09:43:24.126684Z","shell.execute_reply":"2021-10-03T09:43:24.139296Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\nvocab_size = len(word2idx) + 1\nembedding_dim = 256\nhidden_dim = 128\nlr = 3.5e-5\nresults = {x: {} for x in range(1)}\nfold = 0\nbatch_size = 16\nepochs = 15\n\nloss_fcn = nn.NLLLoss()\nbest_valid_loss = float('inf')\nmodel = LstmModel(vocab_size, embedding_dim, hidden_dim)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nbest_train_predictions, best_test_predictions, best_train_labels, best_test_labels = [], [], [], []\nresults[fold]['train_precision'] = []\nresults[fold]['train_recall'] = []\nresults[fold]['train_f1'] = []\nresults[fold]['validation_precision'] = []\nresults[fold]['validation_recall'] = []\nresults[fold]['validation_f1'] = []\ntrain_losses, valid_losses = [], []\ntrain_data_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_data_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\nfor epoch in range(epochs):\n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    # train model\n    train_loss, train_predictions, train_labels, model, optimizer = run_model(model, train_data_loader, loss_fcn, optimizer,\n                                                            gpu, True)\n    # evaluate model\n    valid_loss, test_predictions, test_labels, model, optimizer = run_model(model, test_data_loader, loss_fcn, optimizer,\n                                                          gpu, False)\n    # save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        best_train_predictions = train_predictions\n        best_test_predictions = test_predictions\n        best_train_labels = train_labels\n        best_test_labels = test_labels\n        torch.save(model.state_dict(), f'saved_weights_Fold{fold}.pt')\n        # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    print(f'Losses - Train:{train_loss:.3f} / Validation:{valid_loss:.3f}')\n    results[fold]['train_precision'].append(precision_score(train_labels, train_predictions))\n    results[fold]['train_recall'].append(recall_score(train_labels, train_predictions))\n    results[fold]['train_f1'].append(f1_score(train_labels, train_predictions))\n    results[fold]['validation_precision'].append(precision_score(test_labels, test_predictions))\n    results[fold]['validation_recall'].append(recall_score(test_labels, test_predictions))\n    results[fold]['validation_f1'].append(f1_score(test_labels, test_predictions))\n    torch.cuda.empty_cache()\nprint('On Train Data')\nprint(classification_report(best_train_labels, best_train_predictions))\nprint('On Test Data')\nprint(classification_report(best_test_labels, best_test_predictions))\nresults[fold]['train_losses'] = train_losses\nresults[fold]['validation_losses'] = valid_losses","metadata":{"id":"T8e5ZCXjzdjj","outputId":"ed519bc0-475a-4fa9-f3a4-cd356ae93ea2","execution":{"iopub.status.busy":"2021-10-03T09:50:35.840287Z","iopub.execute_input":"2021-10-03T09:50:35.840672Z","iopub.status.idle":"2021-10-03T09:53:10.508701Z","shell.execute_reply.started":"2021-10-03T09:50:35.840640Z","shell.execute_reply":"2021-10-03T09:53:10.507701Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"plot_results(results, 'LSTM Text Classification')","metadata":{"id":"3eXuxh3U1KGg","execution":{"iopub.status.busy":"2021-10-03T09:53:10.511087Z","iopub.execute_input":"2021-10-03T09:53:10.511418Z","iopub.status.idle":"2021-10-03T09:53:11.135099Z","shell.execute_reply.started":"2021-10-03T09:53:10.511361Z","shell.execute_reply":"2021-10-03T09:53:11.134016Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}